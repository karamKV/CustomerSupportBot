{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48813e-a5c6-41a1-877f-b4e33719dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import sys\n",
    "\n",
    "class URLHierarchyBuilder:\n",
    "    def __init__(self, base_url, max_depth=2, max_urls=500, rate_limit=1):\n",
    "        self.base_url = base_url\n",
    "        self.hierarchy = defaultdict(list)\n",
    "        self.visited_urls = set()\n",
    "        self.max_depth = max_depth\n",
    "        self.max_urls = max_urls\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.lock = threading.Lock()\n",
    "        self.processed_count = 0\n",
    "        self.errors = defaultdict(list)\n",
    "        self.setup_session()\n",
    "\n",
    "    def setup_session(self):\n",
    "        \"\"\"Configure requests session with retry strategy\"\"\"\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "\n",
    "    def rate_limited_request(self, url):\n",
    "        \"\"\"Make a rate-limited request\"\"\"\n",
    "        with self.lock:\n",
    "            current_time = time.time()\n",
    "            time_since_last_request = current_time - self.last_request_time\n",
    "            if time_since_last_request < self.rate_limit:\n",
    "                time.sleep(self.rate_limit - time_since_last_request)\n",
    "            self.last_request_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.errors['request'].append((url, str(e)))\n",
    "            return None\n",
    "\n",
    "    def get_page_links(self, url, current_depth=0):\n",
    "        \"\"\"Extract links from a page\"\"\"\n",
    "        if (current_depth >= self.max_depth or \n",
    "            url in self.visited_urls or \n",
    "            len(self.visited_urls) >= self.max_urls):\n",
    "            return\n",
    "\n",
    "        with self.lock:\n",
    "            if url in self.visited_urls:\n",
    "                return\n",
    "            self.visited_urls.add(url)\n",
    "            self.processed_count += 1\n",
    "            percentage = (self.processed_count / self.max_urls) * 100\n",
    "            print(f\"Processing {self.processed_count}/{self.max_urls} ({percentage:.1f}%): {url} (Depth: {current_depth})\")\n",
    "\n",
    "        response = self.rate_limited_request(url)\n",
    "        if not response:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    absolute_url = urljoin(url, href)\n",
    "                    if self.is_valid_internal_url(absolute_url):\n",
    "                        link_text = link.get_text(strip=True) or absolute_url\n",
    "                        with self.lock:\n",
    "                            self.hierarchy[url].append({\n",
    "                                'url': absolute_url,\n",
    "                                'text': link_text\n",
    "                            })\n",
    "                        if absolute_url not in self.visited_urls:\n",
    "                            self.get_page_links(absolute_url, current_depth + 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.errors['parsing'].append((url, str(e)))\n",
    "\n",
    "    def is_valid_internal_url(self, url):\n",
    "        \"\"\"Check if URL is valid and internal\"\"\"\n",
    "        try:\n",
    "            parsed_base = urlparse(self.base_url)\n",
    "            parsed_url = urlparse(url)\n",
    "            return (parsed_url.netloc == parsed_base.netloc and \n",
    "                   parsed_url.scheme in ['http', 'https'] and\n",
    "                   '#' not in url)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def create_simplified_structure(self):\n",
    "        \"\"\"Create a simplified URL structure\"\"\"\n",
    "        return {\n",
    "            'metadata': {\n",
    "                'base_url': self.base_url,\n",
    "                'total_urls': len(self.visited_urls),\n",
    "                'max_depth': self.max_depth,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            },\n",
    "            'urls': list(self.visited_urls),\n",
    "            'links': {url: [link['url'] for link in links] \n",
    "                     for url, links in self.hierarchy.items()},\n",
    "            'errors': dict(self.errors)\n",
    "        }\n",
    "\n",
    "    def save_results(self, filename='url_structure.json'):\n",
    "        \"\"\"Save crawl results to file\"\"\"\n",
    "        print(\"\\nPreparing to save results...\")\n",
    "        try:\n",
    "            data = self.create_simplified_structure()\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Results successfully saved to '{filename}'\")\n",
    "            \n",
    "            # Save a simple URL list for backup\n",
    "            with open('url_list.txt', 'w', encoding='utf-8') as f:\n",
    "                for url in self.visited_urls:\n",
    "                    f.write(f\"{url}\\n\")\n",
    "            print(\"URL list saved to 'url_list.txt'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            # Emergency save of just the URLs\n",
    "            try:\n",
    "                with open('emergency_url_list.txt', 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(self.visited_urls))\n",
    "                print(\"Emergency URL list saved to 'emergency_url_list.txt'\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Emergency save failed: {e2}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Main crawl method\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting crawl of {self.base_url}\")\n",
    "        print(f\"Max depth: {self.max_depth}, Max URLs: {self.max_urls}\")\n",
    "        \n",
    "        try:\n",
    "            self.get_page_links(self.base_url)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\nCrawl Statistics:\")\n",
    "            print(f\"Total URLs processed: {len(self.visited_urls)}\")\n",
    "            print(f\"Time taken: {duration:.2f} seconds\")\n",
    "            print(f\"Average time per URL: {duration/len(self.visited_urls):.2f} seconds\")\n",
    "            print(f\"Total errors: {sum(len(errors) for errors in self.errors.values())}\")\n",
    "            \n",
    "            self.save_results()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nCrawl interrupted by user\")\n",
    "            self.save_results('interrupted_results.json')\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCrawl failed: {e}\")\n",
    "            self.save_results('error_results.json')\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    base_url = \"BASEURL\"\n",
    "    max_depth = 3  # Limit depth to prevent excessive crawling\n",
    "    max_urls = 1000  # Limit total URLs\n",
    "    rate_limit = 1  # Seconds between requests\n",
    "    \n",
    "    try:\n",
    "        crawler = URLHierarchyBuilder(\n",
    "            base_url=base_url,\n",
    "            max_depth=max_depth,\n",
    "            max_urls=max_urls,\n",
    "            rate_limit=rate_limit\n",
    "        )\n",
    "        \n",
    "        crawler.crawl()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
